# Laâ€¯Rosalina Resort AssistantÂ ğŸ‘‹ğŸ›ï¸

This project is a fully bilingual (EnglishÂ &Â Spanish) assistant that demonstrates how a **LangGraph agent** can combine Retrievalâ€‘Augmented Generation with live tool calls against an existing hotelâ€‘management backend.
The chatbot can explain services, amenities, checkâ€‘in policies, and â€“Â cruciallyÂ â€“ perform **realâ€‘time roomâ€‘availability checks** by hitting the Booking REST API I had built previously (Swagger docs: [http://sdtsoftware-001-site14.atempurl.com/swagger](http://sdtsoftware-001-site14.atempurl.com/swagger)).
All reasoning, retrieval, and toolâ€‘use steps are orchestrated inside a single stateful LangGraph workflow.

## ğŸŒ Public demo

A public demo is running on **Streamlit CommunityÂ Cloud**:

**Live URL:** [https://larosalinaresort.streamlit.app/](https://larosalinaresort.streamlit.app/)

Feel free to try the quickâ€‘suggestion buttons or ask any question about the hotel in either English or Spanish.

---

## âœ¨ Features

| Capability                     | Tech behind it                                               |
| ------------------------------ | ------------------------------------------------------------ |
| Conversational UI              | **Streamlit** `st.chat_*`                                    |
| Routing & orchestration        | **LangGraph** (`StateGraph`)                                 |
| Retrievalâ€‘Augmented Generation | **Pinecone** vector DB + OpenAI embeddings                   |
| Live availability check        | Internal **Booking REST API** (tool use)                     |
| Parallel tool execution        | `RunnableParallel` â†’ `both_node`                             |
| Language autoâ€‘detection        | `langdetect`, forces answer in user language                 |
| Observability                  | **LangSmith** tracing (toggle with `LANGSMITH_TRACING=true`) |

---

## ğŸ–‡ï¸Â Agent architecture & LangGraph flow

The core of this project is a **LangGraph agent**Â â€” an autonomous workflow that decides which tool(s) to call, gathers knowledge, and synthesises the final answer. The agent orchestrates four nodes:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   router_node â”‚  (LLM decides: info / availability / both / end)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ conditional edges
       â”‚           
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ info_node   â”‚      â”‚ availability_nodeâ”‚
â”‚ (RAG over   â”‚      â”‚ (LLM â†’ Booking   â”‚
â”‚  Pinecone)  â”‚      â”‚  API wrapper)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â”‚         â–½ parallel      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º both_node â—„â”€â”€â”€â”˜  (runs the two above in parallel and merges results)
                     â”‚
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ response_node  â”‚  (LLM crafts final answer in the user's language)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Endâ€‘toâ€‘end flow

1. **Streamlit** displays the chat UI and captures the userâ€™s message.
2. The message, plus an empty `GraphState`, is sent to **`graph_runnable.invoke()`**.
3. The **router node** uses GPTâ€‘4 to decide which branch(es) to execute:

   * `info_node` â†’ retrieves docs from the Pinecone index (RAG).
   * `availability_node` â†’ calls the hotel **Booking API** you built previously.
   * `both_node` â†’ runs both in parallel and merges.
4. **response\_node** receives the enriched state (history, documents, availability) and
   returns the final text.
5. Streamlit streams the reply, stores both turns in `InMemoryChatMessageHistory`, and reruns the app so the chat input reâ€‘appears.

> **Why LangGraph?**Â Unlike vanilla LangChain chains, a **LangGraph agent** persists state between nodes, offers firstâ€‘class branching & parallel execution, and integrates perfectly with LangSmith tracing.

---

## ğŸ—‚ï¸ Directory structure

```text
src/
 â”œ app/               # Streamlit frontend
 â”‚   â””â”€ main.py
 â”œ agents/            # LangGraph orchestration
 â”‚   â”œâ”€ graph.py
 â”‚   â”œâ”€ state.py
 â”‚   â”œâ”€ utils/merge.py
 â”‚   â””â”€ nodes/
 â”‚       â”œâ”€ router_node.py
 â”‚       â”œâ”€ info_node.py
 â”‚       â”œâ”€ availability_node.py
 â”‚       â””â”€ response_node.py
 â”œ services/          # Wrapper for Booking API
 â”” config/            # settings.py (reads env vars)
tests/                # pytest cases
requirements.txt
Dockerfile            # local docker build (Streamlit Cloud ignores this)
```

---

## ğŸš€ Quick start (local)

```bash
# 1. clone repo
git clone https://github.com/ezequielsirne/LangGraph-Agent-API.git
cd LangGraph-Agent-API

# 2. create venv & install deps
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 3. set environment variables
cp .env.example .env               # fill with your keys, or export manually
export $(grep -v ^# .env | xargs)  # Linux/macOS only

# 4. run streamlit
streamlit run src/app/main.py
```

Open [http://localhost:8501](http://localhost:8501) âœ chat with the assistant.

---

## ğŸ³Â Run with Docker

```bash
docker build -t larosalina:latest .
docker run --env-file .env -p 8501:8501 larosalina:latest
```

## âš™ï¸ Required environment variables

Create `.env` (or use Streamlit Cloud â€œSecretsâ€) in **TOML** format:

```toml
# OpenAI
OPENAI_API_KEY = "sk-..."
MODEL          = "gpt-4.1-nano"

# Pinecone
PINECONE_API_KEY     = "pcsk_..."
PINECONE_ENVIRONMENT = "us-east-1"
PINECONE_INDEX_NAME  = "langgraph-agent-api"

# LangSmith (optional)
LANGSMITH_TRACING  = true
LANGSMITH_ENDPOINT = "https://api.smith.langchain.com"
LANGSMITH_API_KEY  = "lsv2_..."
LANGSMITH_PROJECT  = "langgraph-agent-api"

# Booking API
BOOKING_API_URL  = "http://sdtsoftware-001-site14.atempurl.com"
BOOKING_API_USER = "admin"
BOOKING_API_PASS = "admin"
```

---

## ğŸ› ï¸ Development tips

* **Hotâ€‘reload**: Streamlit autoâ€‘reloads on file save.
* **Vector index**: First run creates the Pinecone index if it doesn't exist.
* **Testing**: `pytest -q` runs unit tests for nodes and utils.
* **Tracing**: Set `LANGSMITH_TRACING=true` to record every LangGraph run.

---

## ğŸ“„Â License

MIT Â©Â 2025 â€”â€¯Ezequiel Sirne
Feel free to fork, adapt, and improve! PRs & issues welcome.
